{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "Let's take a look at the data we got from the Wikipedia CirrusSearch dump. The gzip 'content' file is 34 GB on disk, so I don't think that we want to try decompressing and reading the whole thing into memory unless we have to. Let's see if we can stream the data from the gzip archive and take a look at what we have.\n",
    "\n",
    "**Estimated total run time**: extraction + cleaning = about 1 hour\n",
    "\n",
    "**TLDR**\n",
    "1. CirrusSearch 'content' dump has text and title keys for 6.9 million articles.\n",
    "2. Reading from decompressed JSON is about seven times faster than reading from the gzip archive.\n",
    "3. Decoding all of the articles to dictionaries and extracting the text should take about 10 minutes.\n",
    "4. mwparserfromhell does a good job of cleaning up the wikicode source, but is slow.\n",
    "5. The text cleaning would take 17 hours with a single worker process, but by batching the data and parallelizing the processing we can get the net time down to about an hour.\n",
    "\n",
    "## 1. Run set-up\n",
    "\n",
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# Standard imports\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from gzip import GzipFile\n",
    "\n",
    "# PyPI imports\n",
    "import mwparserfromhell\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Internal imports\n",
    "import functions.notebook_helper as helper_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data filepaths\n",
    "gzip_data_file_path='./data/raw_data/enwiki-20240930-cirrussearch-content.json.gz'\n",
    "json_data_file_path='./data/raw_data/enwiki-20240930-cirrussearch-content.json'\n",
    "\n",
    "# Number of records to load for inspection\n",
    "sample_records=5\n",
    "\n",
    "# Turn experiments on or off\n",
    "run_full_read=False\n",
    "run_rate_experiments=True\n",
    "run_parse_experiments=True\n",
    "\n",
    "# Run replicates and use the mean rates to estimate the total read times\n",
    "read_time_estimate_replicates=100\n",
    "read_time_estimate_replicate_size=500\n",
    "\n",
    "# Where to save plots\n",
    "figure_dir='./notebooks/figures/01-wikipedia_data_exploration'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Load & inspect sample record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a few records for inspection\n",
    "file_stream=GzipFile(gzip_data_file_path)\n",
    "records = []\n",
    "\n",
    "for i in range(sample_records):\n",
    "\n",
    "    line=next(file_stream)\n",
    "    record=json.loads(line)\n",
    "    records.append(record)\n",
    "\n",
    "print(f'Loaded {len(records)} records from gzip archive.')\n",
    "print(f'Record is: {type(records[0])}')\n",
    "\n",
    "print(f'\\nRecord 0 contains:')\n",
    "\n",
    "for key, value in records[0].items():\n",
    "    print(f' {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks like the first line in the file is just some metadata. Let's look at the second record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nRecord 1 keys:')\n",
    "\n",
    "for key in records[1].keys():\n",
    "    print(f' {key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More like what we were expecting. We have keys for title, text, timestamp, language, even popularity score? Didn't know Wikipedia had that. Here is the title and some text from the first record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_text=' '.join(records[1]['text'].split(' ')[:100])\n",
    "end_text=' '.join(records[1]['text'].split(' ')[-100:])\n",
    "\n",
    "print(f\"Title: {records[1]['title']}\")\n",
    "print(f\"Text:\\n{start_text}\\n...\\n{end_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that's a Wikipedia article. Good, I think we can work with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Record read rate: gzip vs JSON\n",
    "\n",
    "Since the unzipped files are too big to fit in memory, let's first see if there is any time benefit to streaming the data from the gzip archive vs the decompressed JSON file. We have two ways we can read the data:\n",
    "\n",
    "1. From the gzip archive\n",
    "2. From the decompressed JSON\n",
    "\n",
    "Which one is faster? By how much? There is obviously an advantage to reading straight from the archive. Let's read a bunch of records and time it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Dataset size\n",
    "\n",
    "First, read all the data through once to determine how many records we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_full_read is True:\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open(json_data_file_path, 'r') as file:\n",
    "        \n",
    "        # Iterate over the lines\n",
    "        for line_count, line in enumerate(file):\n",
    "\n",
    "            # Print a status update every 1000 lines\n",
    "            if line_count % 1000 == 0:\n",
    "                print(f'Read {line_count} lines', end= '\\r')\n",
    "\n",
    "else:\n",
    "    # Total line count determined from prior full read\n",
    "    line_count=13778448\n",
    "\n",
    "# Divide by two since only every other line actually contains an article text\n",
    "record_count=line_count // 2\n",
    "\n",
    "print(f'{line_count} lines total')\n",
    "print(f'{record_count} article texts\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Gzip archive read rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_rate_experiments is True:\n",
    "\n",
    "    # Collect the read rate in lines per second for each replicate\n",
    "    gzip_read_rates=[]\n",
    "\n",
    "    # Open gzip JSON lines file stream\n",
    "    file=GzipFile(gzip_data_file_path)\n",
    "\n",
    "    # Loop on replicates\n",
    "    while len(gzip_read_rates) < read_time_estimate_replicates:\n",
    "\n",
    "        # Time how long it takes to load each replicate of records\n",
    "        start_time=time.time()\n",
    "\n",
    "        for i in range(read_time_estimate_replicate_size):\n",
    "            line=next(file)\n",
    "        \n",
    "        dT=time.time() - start_time\n",
    "        gzip_read_rates.append(read_time_estimate_replicate_size / dT)\n",
    "\n",
    "    # Print the result\n",
    "    mean_gzip_read_rate=sum(gzip_read_rates) / len(gzip_read_rates)\n",
    "    print(f'Mean gzip read rate: {mean_gzip_read_rate:.0f} lines per second\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. JSON read rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_rate_experiments is True:\n",
    "\n",
    "    # Collect the read rate in lines per second for each replicate\n",
    "    json_read_rates=[]\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open(json_data_file_path, 'r') as file:\n",
    "\n",
    "        # Loop on replicates\n",
    "        while len(json_read_rates) < read_time_estimate_replicates:\n",
    "\n",
    "            # Time how long it takes to load each replicate of records\n",
    "            start_time=time.time()\n",
    "\n",
    "            for i in range(read_time_estimate_replicate_size):\n",
    "                line=next(file)\n",
    "            \n",
    "            dT=time.time() - start_time\n",
    "            json_read_rates.append(read_time_estimate_replicate_size / dT)\n",
    "\n",
    "    # Print the result\n",
    "    mean_json_read_rate=sum(json_read_rates) / len(json_read_rates)\n",
    "    print(f'Mean JSON read rate: {mean_json_read_rate:.0f} lines per second\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Read rate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_rate_experiments is True:\n",
    "\n",
    "    plt.title('Read rate benchmark: file type')\n",
    "\n",
    "    plt.hist(\n",
    "        json_read_rates,\n",
    "        facecolor='green',\n",
    "        label='JSON',\n",
    "        alpha=0.5,\n",
    "        bins=25\n",
    "    )\n",
    "\n",
    "    plt.hist(\n",
    "        gzip_read_rates,\n",
    "        facecolor='blue',\n",
    "        label='gzip',\n",
    "        alpha=0.5,\n",
    "        bins=25\n",
    "    )\n",
    "\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.xlabel(f'Mean replicate read rate (lines per second)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(f'{figure_dir}/2.4-read_rate_file_type.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\nEstimated total gzip read time: {((line_count / mean_gzip_read_rate) / 60):.0f} minutes')\n",
    "    print(f'Estimated JSON read time: {((line_count / mean_json_read_rate) / 60):.0f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from the decompressed JSON file is about 7 times faster reading from the gzip archive. Good news is, both are fast enough to be manageable considering the fact that we really only have to read the data from the stream once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Record parse rate\n",
    "\n",
    "Now let's see how long it takes to read from the decompressed JSON file and actually load the dictionary and pull the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_rate_experiments is True:\n",
    "\n",
    "    # Count line numbers so we can skip odd numbered metadata lines\n",
    "    line_number=0\n",
    "\n",
    "    # Count any key errors we might encounter\n",
    "    keyerror_count=0\n",
    "\n",
    "    # Collect the read rate in records per second for each replicate\n",
    "    parse_rates=[]\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open(json_data_file_path, 'r') as file:\n",
    "\n",
    "        # Loop on replicates\n",
    "        while len(parse_rates) < read_time_estimate_replicates:\n",
    "\n",
    "            # Time how long it takes to load each replicate of records\n",
    "            start_time=time.time()\n",
    "\n",
    "            for i in range(read_time_estimate_replicate_size):\n",
    "\n",
    "                # Get the next line from the stream\n",
    "                line=next(file)\n",
    "\n",
    "                # Skip odd numbered metadata lines\n",
    "                if line_number % 2 != 0:\n",
    "\n",
    "                    # Decode the JSON line to a dictionary\n",
    "                    record=json.loads(line)\n",
    "\n",
    "                    # Get the text from the record, catching key error\n",
    "                    # in case this record doesn't have text for some reason\n",
    "                    try:\n",
    "\n",
    "                        # Only parse namespace 0 articles\n",
    "                        if record['namespace'] == 0:\n",
    "\n",
    "                            text=record['text']\n",
    "\n",
    "                    except KeyError as e:\n",
    "                        keyerror_count+=1\n",
    "\n",
    "                line_number+=1\n",
    "            \n",
    "            # Get and save the mean parse rate for this replicate\n",
    "            dT=time.time() - start_time\n",
    "            parse_rates.append(read_time_estimate_replicate_size / dT)\n",
    "\n",
    "    # Print the result\n",
    "    mean_parse_rate=(sum(parse_rates) / len(parse_rates))\n",
    "    print(f'Total parse time: {((record_count / mean_parse_rate) / 60):.0f} minutes')\n",
    "    print(f'Mean parse rate: {mean_parse_rate:.0f} records per second')\n",
    "    print(f'Encountered {keyerror_count} key errors\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible, actually loading the text from each record slows us down by a factor of 5, but the total time is still manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Additional filtering\n",
    "### 4.1. Wikipedia namespace & category\n",
    "\n",
    "After working with the binned text a bit more, I have noticed that we have a fair amount of disambiguation pages and some others that are not strictly 'articles'. Let's see if we can implement a solution to filter these out ahead to time using the 'namespace' tag from the original record.\n",
    "\n",
    "Here is the list of subject namespaces provided by [Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:What_is_an_article%3F#Namespace)\n",
    "```text\n",
    "Subject namespaces\n",
    "0 \t(Main/Article)\n",
    "2 \tUser\n",
    "4 \tWikipedia\n",
    "6 \tFile\n",
    "8 \tMediaWiki\n",
    "10 \tTemplate\n",
    "12 \tHelp\n",
    "14 \tCategory\n",
    "100 \tPortal\n",
    "118 \tDraft\n",
    "126 \tMOS\n",
    "710 \tTimedText\n",
    "828 \tModule\n",
    "```\n",
    "\n",
    "We pretty obviously want the '0' namespace for Main/Article. However, I'm not sure if taking only that tag will filter out disambiguation pages or not. We also have a 'category' tag. Let's see if we can figure out if there is anything useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_parse_experiments is True:\n",
    "\n",
    "    # Counter for namespace 0 records\n",
    "    namespace_zero_count=0\n",
    "\n",
    "    # Counter for disambiguation pages\n",
    "    disambiguation_count=0\n",
    "\n",
    "    # Count redirect pages\n",
    "    redirects=0\n",
    "\n",
    "    # Outlines\n",
    "    outlines=0\n",
    "\n",
    "    # Lists\n",
    "    lists=0\n",
    "\n",
    "    # Indexes\n",
    "    indexes=0\n",
    "\n",
    "    # Holder for category values\n",
    "    categories=[]\n",
    "\n",
    "    # Track the number of times we catch KeyError, or \n",
    "    # the number of times a record does not have a 'text' key\n",
    "    keyerror_count=0\n",
    "\n",
    "    # Also count the number of texts we were able to successfully extract\n",
    "    text_count=0\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open(json_data_file_path, 'r') as file:\n",
    "        \n",
    "        # Iterate over the lines\n",
    "        for i, line in enumerate(file):\n",
    "\n",
    "            # Skip even numbered header lines and only\n",
    "            # attempt a load on odd numbered article lines\n",
    "            if i % 2 != 0:\n",
    "\n",
    "                # Load the line to JSON\n",
    "                record=json.loads(line)\n",
    "\n",
    "                # Get the text, catching KeyError\n",
    "                try:\n",
    "                    categories.append(record['category'])\n",
    "                    text_count+=1\n",
    "                    \n",
    "                    if record['namespace'] == 0:\n",
    "                        namespace_zero_count+=1\n",
    "\n",
    "                    if 'Disambiguation pages' in record['category']:\n",
    "                        disambiguation_count+=1\n",
    "\n",
    "                    if 'Wikipedia redirects' in record['category']:\n",
    "                        redirects+=1\n",
    "\n",
    "                    if 'Outline' in record['category']:\n",
    "                        outlines+=1\n",
    "\n",
    "                    if 'Lists of topics' in record['category']:\n",
    "                        lists+=1\n",
    "\n",
    "                    if 'Wikipedia indexes' in record['categories']:\n",
    "                        indexes+=1\n",
    "\n",
    "                # Count key errors\n",
    "                except KeyError as e:\n",
    "                    keyerror_count+=1\n",
    "                \n",
    "            # Print a status update every 5000 lines\n",
    "            if i % 5000 == 0:\n",
    "                print(f'Read {i} lines', end= '\\r')\n",
    "\n",
    "            # Break after 100000 records\n",
    "            if i == 100000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_parse_experiments is True:\n",
    "    \n",
    "    print(f'Total text records: {text_count}')\n",
    "    print(f'Namespace 0 text records: {namespace_zero_count}')\n",
    "    print(f'Disambiguation articles: {disambiguation_count}')\n",
    "    print(f'Redirect records: {redirects}')\n",
    "    print(f'Outline records: {outlines}')\n",
    "    print(f'List articles: {lists}')\n",
    "    print(f'Index articles: {indexes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not really helping much. We are already getting only namespace 0 pages. The only other useful category looks like disambiguation - we can exclude those moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Mwparserfromhell\n",
    "\n",
    "Let's try this another way - there is a package available to work with Wikipedia source code: [mwparserfromhell](https://github.com/earwig/mwparserfromhell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_parse_experiments is True:\n",
    "\n",
    "    # Convert source string to wikicode\n",
    "    wikicode=mwparserfromhell.parse(records[1]['source_text'])\n",
    "\n",
    "    # Strip garbage out of wikicode source\n",
    "    source_string=wikicode.strip_code(\n",
    "        normalize=True,\n",
    "        collapse=True,\n",
    "        keep_template_params=False\n",
    "    )\n",
    "\n",
    "\n",
    "    # Remove extra sections from the end of the document\n",
    "    source_string=helper_funcs.remove_extra_sections(source_string)\n",
    "\n",
    "    # Do some string replacements\n",
    "    source_string=helper_funcs.fix_bad_symbols(source_string)\n",
    "\n",
    "    # Clean up newlines\n",
    "    source_string=helper_funcs.clean_newlines(source_string)\n",
    "\n",
    "    # Get rid of image thumbnail lines and leading spaces\n",
    "    source_string=helper_funcs.remove_thumbnails(source_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Raw Wikicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_parse_experiments is True:\n",
    "    print(' '.join(str(wikicode).split(' ')[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_parse_experiments is True:\n",
    "    print(' '.join(source_string.split(' ')[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this looks much better than anything we were going to come up with in the next few days. But how much time is it going to add?\n",
    "\n",
    "### 4.3. Mwparserfromhell: parse rate\n",
    "#### 4.3.1. Benchmark specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mw parser appears to be pretty slow - pick a smaller run size\n",
    "read_time_estimate_replicates=100\n",
    "read_time_estimate_replicate_size=200\n",
    "\n",
    "# Count line numbers so we can skip odd numbered metadata lines\n",
    "line_number=0\n",
    "\n",
    "# Count any key errors we might encounter\n",
    "keyerror_count=0\n",
    "\n",
    "# Collect the read rate in records per second for each chunk\n",
    "parse_rates=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_rate_experiments is True:\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open(json_data_file_path, 'r') as file:\n",
    "\n",
    "        # Loop on replicates\n",
    "        while len(parse_rates) < read_time_estimate_replicates:\n",
    "\n",
    "            # Time how long it takes to load each replicate of records\n",
    "            start_time=time.time()\n",
    "\n",
    "            for i in range(read_time_estimate_replicate_size):\n",
    "\n",
    "                # Get the next line from the stream\n",
    "                line=next(file)\n",
    "\n",
    "                # Skip odd numbered metadata lines\n",
    "                if line_number % 2 != 0:\n",
    "\n",
    "                    # Decode the JSON line to a dictionary\n",
    "                    record=json.loads(line)\n",
    "\n",
    "                    # Get the text from the record, catching key error\n",
    "                    # in case this record doesn't have text for some reason\n",
    "                    try:\n",
    "\n",
    "                        # Only parse namespace 0 articles which are not disambiguation\n",
    "                        if record['namespace'] == 0 and 'Disambiguation pages' not in record['category']:\n",
    "\n",
    "                            # Convert source string to wikicode\n",
    "                            wikicode=mwparserfromhell.parse(record['source_text'])\n",
    "\n",
    "                            # Strip garbage out of wikicode source\n",
    "                            source_string=wikicode.strip_code(\n",
    "                                normalize=True,\n",
    "                                collapse=True,\n",
    "                                keep_template_params=False\n",
    "                            )\n",
    "\n",
    "                            # Remove extra sections from the end of the document\n",
    "                            source_string=helper_funcs.remove_extra_sections(source_string)\n",
    "\n",
    "                            # Do some string replacements\n",
    "                            source_string=helper_funcs.fix_bad_symbols(source_string)\n",
    "\n",
    "                            # Clean up newlines\n",
    "                            source_string=helper_funcs.clean_newlines(source_string)\n",
    "\n",
    "                            # Get rid of image thumbnail lines and leading spaces\n",
    "                            source_string=helper_funcs.remove_thumbnails(source_string)\n",
    "\n",
    "                    except KeyError as e:\n",
    "                        keyerror_count+=1\n",
    "\n",
    "                line_number+=1\n",
    "            \n",
    "            # Get and save the mean parse rate for this replicate\n",
    "            dT=time.time() - start_time\n",
    "            parse_rates.append(read_time_estimate_replicate_size / dT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_rate_experiments is True:\n",
    "\n",
    "    plt.hist(parse_rates)\n",
    "    plt.title('Parse rate benchmark: mwparser')\n",
    "    plt.xlabel(f'Parse rate (records per second)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(f'{figure_dir}/4.3.3-parse_rate_mwparser.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    # Print the result\n",
    "    mean_parse_rate=(sum(parse_rates) / len(parse_rates))\n",
    "    print(f'Total parse time: {((record_count / mean_parse_rate) / (60**2)):.1f} hours')\n",
    "    print(f'Mean parse rate: {mean_parse_rate:.0f} records per second')\n",
    "    print(f'Encountered {keyerror_count} key errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Reading from uncompressed JSON is ~7x faster than reading from the Gzip archive directly. Loading the JSON into a dictionary slows it down ~5x to approximately 10 minutes to read the whole file; still manageable.\n",
    "\n",
    "Data cleaning strategy works well:\n",
    "\n",
    "1. Namespaces and categories to get only article pages which are not disambiguation.\n",
    "2. Mwparser to get and clean Wikicode source.\n",
    "3. Custom functions to remove unwanted article sections and do some character level cleanup.\n",
    "\n",
    "Problem that now needs to be solved (or not) is the parse time. With mwparser and the custom functions we are looking at about 17 hours to extract the data. It's single threaded and clearly CPU limited, so we could speed it up by parallelizing it over 18-20 cores. In theory, we should be able to get it under an hour. \n",
    "\n",
    "The other option is to save the coding headache by ignoring it, finish developing the pipeline with a sample of the data and let it run over a long weekend or something. Remember, we only need to extract once. So, our time might be better spent on doing other things.\n",
    "\n",
    "However, the performance nerd in me says go for it...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
