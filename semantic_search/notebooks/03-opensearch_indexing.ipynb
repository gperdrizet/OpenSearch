{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch indexing\n",
    "\n",
    "ETL pipeline is done - but indexing appears painfully slow. From some quick observations with htop and bmon, the bottleneck seems to be OpenSearch calculating the embeddings. But, that's an assumption we can test. Let's make some performance measurements and see what we are working with.\n",
    "\n",
    "## 1. Run set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/opensearch/semantic_search\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# Standard imports\n",
    "import time\n",
    "\n",
    "# PyPI imports\n",
    "import h5py\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "import functions.notebook_helper as helper_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for the text index to insert to\n",
    "knn_index_name='wikipedia-knn'\n",
    "text_index_name='wikipedia-text-only'\n",
    "\n",
    "# Total record count determined in data exploration notebook\n",
    "record_count=13778448\n",
    "\n",
    "# Transformed wikipedia input data\n",
    "input_file_path=f\"{config.DATA_PATH}/wikipedia/{config.TRANSFORMED_TEXT}\"\n",
    "\n",
    "# Initialize the OpenSearch client\n",
    "client=helper_funcs.start_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. OpenSearch initialization\n",
    "\n",
    "Let's make two indexes for comparison: one KNN index with an embedding ingest pipeline and one vanilla text index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KNN index\n",
    "knn_index_body={\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 3,\n",
    "        \"index.knn\": \"true\",\n",
    "        \"default_pipeline\": f'{config.INGEST_PIPELINE_ID}'\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text_embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"method\": {\n",
    "                \"engine\": \"lucene\",\n",
    "                \"space_type\": \"l2\",\n",
    "                \"name\": \"hnsw\",\n",
    "                \"parameters\": {}\n",
    "                }\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "helper_funcs.initialize_index(knn_index_name, knn_index_body)\n",
    "\n",
    "# Create the text index\n",
    "text_index_body={\n",
    "    'settings': {\n",
    "        'index': {\n",
    "            'number_of_shards': 3\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "helper_funcs.initialize_index(text_index_name, text_index_body)\n",
    "# Initialize the OpenSearch client\n",
    "client=helper_funcs.start_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Index insert rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of batches to use for insert rate estimation\n",
    "sample_batches=3\n",
    "\n",
    "# Counter for total records inserted\n",
    "record_num=0\n",
    "\n",
    "# Collector for insert times\n",
    "knn_insert_times=[]\n",
    "text_insert_times=[]\n",
    "\n",
    "# Open a connection to transformed Wikipedia data\n",
    "input_data=h5py.File(input_file_path, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect insert performance data for text only and embedding indexes at the same time by inserting to both in the same loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop on the batches\n",
    "for i in range(sample_batches):\n",
    "\n",
    "    # Load the batch\n",
    "    batch=input_data[f'batches/{i + 1}']\n",
    "\n",
    "    # Loop on the records in the batch\n",
    "    for record in batch:\n",
    "\n",
    "        record_num+=1\n",
    "\n",
    "        # Decode the text\n",
    "        text=record.decode('utf-8')\n",
    "\n",
    "        # Build the requests\n",
    "        knn_request=[]\n",
    "        text_request=[]\n",
    "\n",
    "        knn_request_header={\n",
    "            'update': {\n",
    "                '_index': knn_index_name,\n",
    "                '_id': record_num\n",
    "            }\n",
    "        }\n",
    "\n",
    "        text_request_header={\n",
    "            'update': {\n",
    "                '_index': text_index_name,\n",
    "                '_id': record_num\n",
    "            }\n",
    "        }\n",
    "\n",
    "        knn_request.append(knn_request_header)\n",
    "        text_request.append(text_request_header)\n",
    "\n",
    "        request_body={\n",
    "            'doc': {\n",
    "                'text': text\n",
    "            },\n",
    "            'doc_as_upsert': 'true'\n",
    "        }\n",
    "\n",
    "        knn_request.append(request_body)\n",
    "        text_request.append(request_body)\n",
    "\n",
    "        # Insert the record using the bulk interface. We are\n",
    "        # indexing records one at a time here, but use bulk \n",
    "        # for consistency\n",
    "\n",
    "        # Start the timer\n",
    "        start_time=time.time()\n",
    "\n",
    "        _=client.bulk(knn_request)\n",
    "\n",
    "        # Stop the timer and collect the time\n",
    "        dT=time.time() - start_time\n",
    "        knn_insert_times.append(dT)\n",
    "\n",
    "        # Start the timer\n",
    "        start_time=time.time()\n",
    "\n",
    "        _=client.bulk(text_request)\n",
    "\n",
    "        # Stop the timer and collect the time\n",
    "        dT=time.time() - start_time\n",
    "        text_insert_times.append(dT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text only insert time: 1 days\n",
      "Text only insert rate: 115.713 records per second\n",
      "\n",
      "Total KNN insert time: 202 days\n",
      "KNN insert rate: 0.791 records per second\n"
     ]
    }
   ],
   "source": [
    "mean_text_insert_time=sum(text_insert_times)/len(text_insert_times)\n",
    "text_insert_rate=1 / mean_text_insert_time\n",
    "print(f'Total text only insert time: {(record_count * mean_text_insert_time) / (60*60*24):.0f} days')\n",
    "print(f'Text only insert rate: {text_insert_rate:.3f} records per second\\n')\n",
    "\n",
    "mean_knn_insert_time=sum(knn_insert_times)/len(knn_insert_times)\n",
    "knn_insert_rate=1 / mean_knn_insert_time\n",
    "print(f'Total KNN insert time: {(record_count * mean_knn_insert_time) / (60*60*24):.0f} days')\n",
    "print(f'KNN insert rate: {knn_insert_rate:.3f} records per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes, that's not going to work. Looking at the difference in insert rates, it is clear that we were right about the bottleneck: it's definitely the embedding calculation. So, a few ideas here.\n",
    "\n",
    "1. Though the GPUs are available inside the OpenSearch node Docker containers, OpenSearch is not using them... we should troubleshoot that.\n",
    "2. We can insert batches with the bulk interface - if that means OpenSearch is calculating the embeddings in parallel, using a larger batch size might speed things up.\n",
    "3. We could use multiple insert workers - just looking at htop, we are using less than 50% of available CPU resources while inserting, so we should be able to go faster.\n",
    "4. Last would be reading up on OpenSearch cluster management in general to see if there are settings we should be tuning for this type of thing.\n",
    "\n",
    "Think that getting the GPUs online is going to be the only real solution here. Given the amount of CPU we are already using while making/inserting embeddings we probably can't speed up much more than two-fold through parallelism alone.\n",
    "\n",
    "Let's at least try bigger bulk batches first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
