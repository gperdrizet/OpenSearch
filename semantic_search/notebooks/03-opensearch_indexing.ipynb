{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch indexing\n",
    "\n",
    "ETL pipeline is done - but indexing appears painfully slow. From some quick observations with htop and bmon, the bottleneck seems to be OpenSearch calculating the embeddings. But, that's an assumption we can test. Let's make some performance measurements and see what we are working with.\n",
    "\n",
    "**Estimated total run time**: extraction + cleaning + splitting + embedding + indexing = approximatly 5 days\n",
    "\n",
    "**TLDR**\n",
    "1. GPU support in OpenSearch is not great and using the built in text embedding processor as part of an ingest pipeline on CPU is prohibitively slow, about 200 days to process the whole corpus.\n",
    "2. Calculating the embeddings with HuggingFace transformers is manageable - about 4 days for a single worker.\n",
    "3. Indexing pre-calculated embeddings into a KNN index is comparatively fast - about 6 hours with a bulk insert batch size of 256.\n",
    "\n",
    "## 1. Run set-up\n",
    "\n",
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# Standard imports\n",
    "import time\n",
    "import random\n",
    "\n",
    "# PyPI imports\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "import functions.notebook_helper as helper_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which tests to run\n",
    "run_indexing_rate_benchmark=False\n",
    "run_bulk_indexing_rate_benchmark=False\n",
    "pre_embedding_indexing_rate=True\n",
    "\n",
    "# Estimated total chunks after semantic splitting, determined in\n",
    "# semantic splitting notebook\n",
    "estimated_total_chunks=20648877\n",
    "\n",
    "# Where to save plots\n",
    "figure_dir='./notebooks/figures/03-opensearch_indexing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. OpenSearch initialization\n",
    "\n",
    "Let's make two indexes for comparison: one KNN index with an embedding ingest pipeline and one vanilla text index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for the test indexes to insert to\n",
    "knn_index_name='wikipedia-knn'\n",
    "text_index_name='wikipedia-text-only'\n",
    "\n",
    "# Initialize target indexes for text only and vector KNN\n",
    "helper_funcs.initialize_index(knn_index_name,helper_funcs.KNN_INDEX_BODY)\n",
    "helper_funcs.initialize_index(text_index_name, helper_funcs.TEXT_INDEX_BODY)\n",
    "\n",
    "# Initialize the OpenSearch client\n",
    "client=helper_funcs.start_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Data loading\n",
    "\n",
    "Let's pre-ingest the data into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a connection to transformed Wikipedia data on disk\n",
    "input_file_path=f'{config.DATA_PATH}/wikipedia-sample/{config.PARSED_TEXT}'\n",
    "input_data=h5py.File(input_file_path, 'r')\n",
    "\n",
    "# Holder for the data\n",
    "records=[]\n",
    "\n",
    "# Loop on the input batches\n",
    "for i in input_data['batches']:\n",
    "\n",
    "    # Load the batch\n",
    "    batch=input_data[f'batches/{i}']\n",
    "\n",
    "    # Loop on the records in the batch\n",
    "    for record in batch:\n",
    "\n",
    "        # Collect the text\n",
    "        records.append(record )\n",
    "\n",
    "print(f'Have {len(records)} input records')\n",
    "\n",
    "# Close data connection\n",
    "input_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Indexing rate: text only vs OpenSearch KNN embedding\n",
    "\n",
    "Test batch replicates, time how long it takes to insert each record in the batch and then average the indexing time across the batch.\n",
    "\n",
    "### 2.1. Benchmark specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of batch replicates to use for indexing rate estimation\n",
    "replicates=10\n",
    "\n",
    "# Number of records to sample for each batch\n",
    "batch_size=10\n",
    "\n",
    "# Collectors for indexing rates\n",
    "knn_indexing_rates=[]\n",
    "text_indexing_rates=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Benchmark\n",
    "\n",
    "Collect insert performance data for text only and KNN embedding indexes at the same time by inserting to both in the same loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_indexing_rate_benchmark is True:\n",
    "\n",
    "    # Loop on the replicates\n",
    "    for replicate in range(replicates):\n",
    "\n",
    "        batch=random.sample(records, batch_size)\n",
    "\n",
    "        # Loop on the records in the batch\n",
    "        for record_num, record in enumerate(batch):\n",
    "\n",
    "            # Decode the text\n",
    "            text=record.decode('utf-8')\n",
    "\n",
    "            # Build the requests\n",
    "            knn_request=[]\n",
    "            text_request=[]\n",
    "\n",
    "            knn_request_header={'create': {'_index': knn_index_name,'_id': record_num}}\n",
    "            text_request_header={'create': {'_index': text_index_name,'_id': record_num}}\n",
    "\n",
    "            knn_request.append(knn_request_header)\n",
    "            text_request.append(text_request_header)\n",
    "\n",
    "            request_body={'text': text}\n",
    "            \n",
    "            knn_request.append(request_body)\n",
    "            text_request.append(request_body)\n",
    "\n",
    "            # Insert the record using the bulk interface. We are\n",
    "            # indexing records one at a time here, but use bulk \n",
    "            # for consistency\n",
    "\n",
    "            # Start the timer\n",
    "            start_time=time.time()\n",
    "\n",
    "            _=client.bulk(knn_request)\n",
    "\n",
    "            # Stop the timer and collect the time\n",
    "            dT=time.time() - start_time\n",
    "            knn_indexing_rates.append(dT)\n",
    "\n",
    "            # Start the timer\n",
    "            start_time=time.time()\n",
    "\n",
    "            _=client.bulk(text_request)\n",
    "\n",
    "            # Stop the timer and collect the time\n",
    "            dT=time.time() - start_time\n",
    "            text_indexing_rates.append(1 / dT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Results\n",
    "\n",
    "Now, make a plot of the timing results and estimate how long it would take to index the complete Wikipedia corpus with either indexing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_indexing_rate_benchmark is True:\n",
    "    plt.title('Indexing rate benchmark: index type')\n",
    "\n",
    "    # Use the same set of bins for each dataset\n",
    "    _, bins = np.histogram(knn_indexing_rates + text_indexing_rates, bins=30)\n",
    "\n",
    "    plt.hist(\n",
    "        knn_indexing_rates,\n",
    "        facecolor='green',\n",
    "        label='KNN embedding index',\n",
    "        bins=bins\n",
    "    )\n",
    "\n",
    "    plt.hist(\n",
    "        text_indexing_rates,\n",
    "        facecolor='blue',\n",
    "        label='Text only index',\n",
    "        bins=bins\n",
    "    )\n",
    "\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.xlabel(f'Rate (records per second)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(f'{figure_dir}/2.3-indexing_rate_index_type.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    mean_text_insert_rate=sum(text_indexing_rates)/len(text_indexing_rates)\n",
    "    estimated_total_text_only_insert_time=estimated_total_chunks / mean_text_insert_rate\n",
    "    print(f'Estimated total text only indexing time: {estimated_total_text_only_insert_time / (60*60):.1f} hours')\n",
    "    print(f'Mean text only indexing rate: {mean_text_insert_rate:.0f} records per second\\n')\n",
    "\n",
    "    mean_knn_insert_rate=sum(knn_indexing_rates)/len(knn_indexing_rates)\n",
    "    estimated_total_knn_only_insert_time=estimated_total_chunks / mean_knn_insert_rate\n",
    "    print(f'Estimated total KNN indexing time: {estimated_total_knn_only_insert_time / (60*60*24):.0f} days')\n",
    "    print(f'Mean KNN indexing rate: {mean_knn_insert_rate:.3f} records per second\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes, that's not going to work. Looking at the difference in insert rates, it is clear that we were right about the bottleneck - definitely the embedding calculation/KNN indexing. So, a few ideas here.\n",
    "\n",
    "1. Though the GPUs are available inside the OpenSearch node Docker containers, OpenSearch is not using them... we should troubleshoot that.\n",
    "2. We can insert batches with the bulk interface - if that means OpenSearch is calculating the embeddings in parallel, using a larger batch size might speed things up.\n",
    "3. We could use multiple insert workers - just looking at htop, we are using less than 50% of available CPU resources while inserting, so we should be able to go faster.\n",
    "4. Last would be reading up on OpenSearch cluster management in general to see if there are settings we should be tuning for this type of thing.\n",
    "\n",
    "Think that getting the GPUs online is going to be the only real solution here. Given the amount of CPU we are already using while making/inserting embeddings we probably can't speed up much more than two-fold through parallelism alone.\n",
    "\n",
    "Let's at least try bigger bulk batches first.\n",
    "\n",
    "## 3. Insert rate: bulk insert batch size\n",
    "\n",
    "Run a few replicates of increasing batch size. Time indexing records in each batch replicate using OpenSearch's bulk interface. Calculate the indexing rate as batch size divided by batch time.\n",
    "\n",
    "### 3.1. Benchmark specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk insert batch sizes to test\n",
    "batch_sizes=[1,4,16,64,256]\n",
    "\n",
    "# Replicates for each batch size\n",
    "replicates=10\n",
    "\n",
    "# Holder for results\n",
    "results={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Benchmark\n",
    "\n",
    "Only need to test the KNN index rates here - that's what we are really after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_bulk_indexing_rate_benchmark is True:\n",
    "\n",
    "    # Reinitialize the KNN index\n",
    "    helper_funcs.initialize_index(knn_index_name, helper_funcs.KNN_INDEX_BODY)\n",
    "\n",
    "    # Loop on the batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f'Running replicates for batch size {batch_size}')\n",
    "\n",
    "        # Add an empty list to collect the results, using the batch size as key\n",
    "        results[f'{batch_size}']=[]\n",
    "\n",
    "        for replicate in range(replicates):\n",
    "\n",
    "            batch=random.sample(records, batch_size)\n",
    "\n",
    "            # Loop on the records in the batch to build the indexing requests\n",
    "            knn_requests=[]\n",
    "\n",
    "            for record_num, record in enumerate(batch):\n",
    "\n",
    "                knn_request_header={'create': {'_index': knn_index_name,'_id': record_num}}\n",
    "                knn_requests.append(knn_request_header)\n",
    "\n",
    "                # Record text comes from hdf5 data store as bytes, decode it\n",
    "                text=record.decode('utf-8')\n",
    "\n",
    "                request_body={'text': text}\n",
    "                knn_requests.append(request_body)\n",
    "\n",
    "                # Insert the record using the bulk interface. We are\n",
    "                # indexing records one at a time here, but use bulk \n",
    "                # for consistency\n",
    "\n",
    "                # Start the timer\n",
    "                start_time=time.time()\n",
    "\n",
    "                _=client.bulk(knn_requests)\n",
    "\n",
    "                # Stop the timer and collect the insert rate\n",
    "                dT=time.time() - start_time\n",
    "                results[f'{batch_size}'].append(batch_size / dT)\n",
    "\n",
    "                # Reset the bulk insert batch for the next round\n",
    "                bulk_insert_batch=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Results\n",
    "\n",
    "Plot indexing rate as a function of bulk indexing batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_bulk_indexing_rate_benchmark is True:\n",
    "    \n",
    "    plt.title('Indexing rate benchmark: OpenSearch text embedding ingest pipeline')\n",
    "    plt.xlabel('Bulk insert batch size (records)')\n",
    "    plt.ylabel('Rate (records per second)')\n",
    "\n",
    "    standard_deviations=[]\n",
    "    means=[]\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        times=results[f'{batch_size}']\n",
    "        means.append(np.mean(times))\n",
    "        standard_deviations.append(np.std(times))\n",
    "\n",
    "    plt.errorbar(\n",
    "        batch_sizes, \n",
    "        means, \n",
    "        yerr=standard_deviations, \n",
    "        linestyle='dotted',\n",
    "        marker='o', \n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(f'{figure_dir}/3.3-indexing_rate_bulk_insert.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    mean_indexing_rate=sum(results['256']) / len(results['256'])\n",
    "    print(f'Estimated total indexing time: {(estimated_total_chunks / mean_indexing_rate) / (60*60*24):.1f} days for batch size of 256')\n",
    "    print(f'Mean indexing rate: {mean_indexing_rate:.0f} records per second for batch size of 256')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, cool, looking at the graph, we can get a speed-up of on the order or 10x by increasing the bulk insert batch size. The gains start to saturate around a batch size of 64. While it is the ~10x speed up over single inserts to the KNN index that we were expecting - it is still going to take over three weeks to index the embeddings for all of Wikipedia. No more benefit to be had from parallelism here - CPU is pinned at 100% utilization on all cores. In fact, I'd like to cut it back, so we can use the machine for other things during the run. Either way, we need another solution.\n",
    "\n",
    "Two options I can think of:\n",
    "\n",
    "1. Get the GPUs working with OpenSearch in Docker.\n",
    "2. Pre-calculate the embeddings ourselves using the GPUs and then index them to OpenSearch.\n",
    "\n",
    "After some more reading, it looks like we need to go with option 2. Turns out, GPU support is experimental and requires CUDA 11.6, and it's recommended to run on an Amazon EC2 instance with Neuron. Option 2 is much more flexible and will give us more control over the embedding calculation.\n",
    "\n",
    "## 4. Embedding calculation rate: HF transformers\n",
    "\n",
    "Let's try it with the same model we were using in OpenSearch following the basic instructions in the [HuggingFace model card](https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b). Looks like the model can take a list of document texts for embedding, so let's make the first experiment the embedding batch size.\n",
    "\n",
    "The plan will be to time how long it takes to embed some sample size of texts using different batch sizes. We won't include the model loading or other start-up overhead in the time - during the real run, we will be doing many hours of GPU compute per workunit so any startup time is insignificant in comparison.\n",
    "\n",
    "### 4.1. Benchmark specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to run and where to run it\n",
    "model_name='sentence-transformers/msmarco-distilbert-base-tas-b'\n",
    "gpu='cuda:0'\n",
    "\n",
    "# Number texts to encode for each replicate of each batch size\n",
    "target_texts=3200\n",
    "\n",
    "# Batch sizes to test\n",
    "batch_sizes=[1,2,4,8,16,32]\n",
    "\n",
    "# Number of replicates to run for each batch size\n",
    "replicates=3\n",
    "\n",
    "# Holder for results\n",
    "results={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if pre_embedding_indexing_rate is True:\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "    model=AutoModel.from_pretrained(model_name, device_map=gpu)\n",
    "\n",
    "    # Loop on batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f'Running batch size {batch_size}')\n",
    "\n",
    "        # Add an empty list to collect the results, using the batch size as key\n",
    "        results[f'{batch_size}']=[]\n",
    "\n",
    "        # Calculate how many batches of batch size we need \n",
    "        # to get the target number of texts\n",
    "        num_batches=target_texts // batch_size\n",
    "\n",
    "        # Loop on the replicates\n",
    "        for replicate in range(replicates):\n",
    "\n",
    "            # Generate random batches of texts for this replicate\n",
    "            batches=[]\n",
    "\n",
    "            for i in range(num_batches):\n",
    "\n",
    "                batches.append(random.sample(records, batch_size))\n",
    "\n",
    "            # Collector for embedded texts\n",
    "            embedded_texts=[]\n",
    "\n",
    "            # Start the timer\n",
    "            start_time=time.time()\n",
    "\n",
    "            # Loop on the batches and embed them\n",
    "            for batch in batches:\n",
    "\n",
    "                # Decode each record in the batch\n",
    "                texts=[record.decode('utf-8') for record in batch]\n",
    "\n",
    "                # Tokenize the sample\n",
    "                encoded_input=tokenizer(\n",
    "                    texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                ).to('cuda:0')\n",
    "\n",
    "                # Compute token embeddings\n",
    "                with torch.no_grad():\n",
    "                    model_output=model(**encoded_input, return_dict=True)\n",
    "\n",
    "                # Perform pooling\n",
    "                embeddings=model_output.last_hidden_state[:,0]\n",
    "\n",
    "                # Collect the embeddings\n",
    "                embedded_texts.extend(embeddings.tolist())\n",
    "\n",
    "            # Stop the timer and collect the timing data\n",
    "            dT=time.time() - start_time\n",
    "            results[f'{batch_size}'].append(len(embedded_texts) / dT)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Results\n",
    "\n",
    "Batch size of 128 runs, at batch size of 256 we get OOMs on the 8 GB GTX1070."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_embedding_indexing_rate is True:\n",
    "\n",
    "    plt.title('Embedding rate benchmark: batch size')\n",
    "    plt.xlabel('Batch size (records)')\n",
    "    plt.ylabel('Embedding calculation rate (records per second)')\n",
    "\n",
    "    standard_deviations=[]\n",
    "    means=[]\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        times=results[f'{batch_size}']\n",
    "        means.append(np.mean(times))\n",
    "        standard_deviations.append(np.std(times))\n",
    "\n",
    "    plt.errorbar(\n",
    "        batch_sizes, \n",
    "        means, \n",
    "        yerr=standard_deviations, \n",
    "        linestyle='dotted',\n",
    "        marker='o', \n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "    plt.savefig(f'{figure_dir}/4.3-embedding_rate_batch_size.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    mean_embedding_rate=sum(results['1']) / len(results['1'])\n",
    "    print(f'Estimated total embedding time: {(estimated_total_chunks / mean_embedding_rate) / (60*60*24):.1f} days for batch size of 1')\n",
    "    print(f'Mean embedding rate: {mean_embedding_rate:.0f} records per second for batch size of 1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, interesting. Not the result I was expecting, but useful none-the-less. Using larger batch sizes slows down the net embedding rate. We can definitely embed texts much faster this way than using an OpenSearch ingest pipeline, especially using a batch size of one.\n",
    "\n",
    "Remember: we still need to insert the embeddings. Let's hope the performance of batch inserting pre-calculated embeddings to a KNN index is more similar to that of indexing plain text than using an embedding ingest pipeline...\n",
    "\n",
    "## 5. KNN indexing rate: pre-calculated embeddings\n",
    "\n",
    "### 5.1. Benchmark specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk insert batch sizes to test\n",
    "batch_sizes=[1,2,4,8,16,32,64,128,256]\n",
    "\n",
    "# Replicates for each batch size\n",
    "replicates=3\n",
    "\n",
    "# Holder for results\n",
    "results={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### 5.2. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if pre_embedding_indexing_rate is True:\n",
    "\n",
    "    # Initialize a KNN index without an OpenSearch ingest pipeline\n",
    "    helper_funcs.initialize_index(knn_index_name, helper_funcs.PRE_EMBEDDED_KNN_INDEX)\n",
    "\n",
    "    # Loop on batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f'Running replicates for batch size {batch_size}')\n",
    "\n",
    "        # Add the batch size to the results\n",
    "        results[f'{batch_size}']=[]\n",
    "\n",
    "        # Start an empty collector for the insert batch\n",
    "        bulk_insert_batch=[]\n",
    "\n",
    "        for replicate in range(replicates):\n",
    "\n",
    "            # Grab a random batch of texts to embed\n",
    "            embedding_batch=random.sample(embedded_texts, batch_size)\n",
    "\n",
    "            # Build the requests\n",
    "            knn_requests=[]\n",
    "\n",
    "            for record_num, embedding in enumerate(embedding_batch):\n",
    "\n",
    "                knn_request_header={'create': {'_index': knn_index_name,'_id': record_num}}\n",
    "                knn_requests.append(knn_request_header)\n",
    "\n",
    "                request_body={'text_embedding': embedding}\n",
    "                knn_requests.append(request_body)\n",
    "\n",
    "            # Insert the record using the bulk interface. We are\n",
    "            # indexing records one at a time here, but use bulk \n",
    "            # for consistency\n",
    "\n",
    "            # Start the timer\n",
    "            start_time=time.time()\n",
    "\n",
    "            _=client.bulk(knn_requests)\n",
    "\n",
    "            # Stop the timer and collect the insert rate\n",
    "            dT=time.time() - start_time\n",
    "            results[f'{batch_size}'].append(batch_size / dT)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_embedding_indexing_rate is True:\n",
    "    \n",
    "    plt.title('Indexing rate benchmark: KNN index, pre-calculated embeddings')\n",
    "    plt.xlabel('Batch size (records)')\n",
    "    plt.ylabel('Insert rate (records per second)')\n",
    "\n",
    "    standard_deviations=[]\n",
    "    means=[]\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        times=results[f'{batch_size}']\n",
    "        means.append(np.mean(times))\n",
    "        standard_deviations.append(np.std(times))\n",
    "\n",
    "    plt.errorbar(\n",
    "        batch_sizes, \n",
    "        means, \n",
    "        yerr=standard_deviations, \n",
    "        linestyle='dotted',\n",
    "        marker='o', \n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(f'{figure_dir}/5.3-indexing_rate_pre-embedded.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    mean_indexing_rate=sum(results['256']) / len(results['256'])\n",
    "    print(f'Estimated total indexing time: {(estimated_total_chunks / mean_indexing_rate) / (60*60):.1f} hours for batch size of 256')\n",
    "    print(f'Mean indexing rate: {mean_indexing_rate:.0f} records per second for batch size of 256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! OK, I think we have it - about four days to calculate the embeddings and four and a half hours to index them is way better than over 200 days using an OpenSearch text embedding pipeline. We probably can speed up the embedding calculation a bit more by parallelizing it over the GPUs.\n",
    "\n",
    "## 6. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
