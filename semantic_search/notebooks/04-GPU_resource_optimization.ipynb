{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU resource optimization\n",
    "\n",
    "Currently, the longest run time in the pipeline is calculating the embeddings with HuggingFace transformers and msmarco-distilbert-base-tas-b. Should probably read up on embedding models - it may be possible to pick a better one. We are only using this one because it was recommended by OpenSearch for their text embedding ingest pipeline. Since we are now calculating the embedding ourselves outside of OpenSearch we can use whatever we want.\n",
    "\n",
    "But before we start swapping models, I want to try the embedding run with multiple GPU workers, both on the same GPU and across multiple GPUs.\n",
    "We know from the OpenSearch indexing notebook that in a single process, an embedding batch size of 1 text chunk is the fastest. This simplifies the batch accumulation and submission somewhat. It also means that the GPU has memory to spare and could fit multiple copies of the model for multiple parallel embedding jobs.\n",
    "\n",
    "Need to be careful to time only the embedding part of the job submission loop. This is done to match the single process benchmark from the OpenSearch indexing notebook. We also don't want/need to include the spin-up time because during the real run, we will have hours or days of GPU compute time and any initialization overhead will be insignificant in comparison.\n",
    "\n",
    "## 1. Run set-up\n",
    "\n",
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# Standard imports\n",
    "import time\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Manager, Process\n",
    "\n",
    "# PyPI imports\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "import notebooks.notebook_helper as helper_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run or skip experiments\n",
    "run_multiworker_benchmark=True\n",
    "run_multigpu_benchmark=True\n",
    "run_multigpu_queue_benchmark=True\n",
    "\n",
    "# Estimated total chunks after semantic splitting, determined in\n",
    "# semantic splitting notebook\n",
    "estimated_total_chunks=20648877\n",
    "\n",
    "# Where to save plots\n",
    "figure_dir='./notebooks/figures/04-GPU_resource_optimization'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a connection to the input data on disk\n",
    "input_file_path=f'{config.DATA_PATH}/wikipedia-sample/{config.PARSED_TEXT}'\n",
    "input_data=h5py.File(input_file_path, 'r')\n",
    "\n",
    "# Ingest all of the parsed sample data so we don't have to mess around\n",
    "# with accumulating batches for the workers from input batches\n",
    "records=[]\n",
    "\n",
    "# Loop on the batches\n",
    "for batch_num in input_data['batches']:\n",
    "\n",
    "    # Grab the batch from the hdf5 connection\n",
    "    batch=input_data[f'batches/{batch_num}']\n",
    "\n",
    "    # Add record to batch and count\n",
    "    for record in batch:\n",
    "        records.append(record)\n",
    "\n",
    "# Close the connection to the input data\n",
    "input_data.close()\n",
    "\n",
    "print(f'Have {len(records)} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple workers, single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Benchmark specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU to assign work to\n",
    "worker_gpu=['cuda:0']\n",
    "\n",
    "# Numbers of workers per GPU to test\n",
    "gpu_worker_counts=[1,2,4,8]\n",
    "\n",
    "# Number texts to encode for each replicate of each worker count\n",
    "target_texts=3200\n",
    "\n",
    "# Number of replicates to run for each worker count\n",
    "replicates=3\n",
    "\n",
    "# Build holder for results\n",
    "results={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_multiworker_benchmark == True:\n",
    "\n",
    "    # Loop on the worker counts\n",
    "    for n_workers in gpu_worker_counts:\n",
    "        print(f'Embedding with {n_workers} GPU workers.')\n",
    "        \n",
    "        # Add an empty list to collect the results, using the worker count as key\n",
    "        results[f'{n_workers}']=[]\n",
    "\n",
    "        # Calculate how many texts we need to send to each worker\n",
    "        # the get the target number of texts\n",
    "        texts_per_worker=target_texts // n_workers\n",
    "\n",
    "        # Build the GPU list for this worker count\n",
    "        worker_gpus=worker_gpu*n_workers\n",
    "\n",
    "        for replicate in range(replicates):\n",
    "\n",
    "            # Generate random batches of texts for each worker\n",
    "            batches=[]\n",
    "\n",
    "            for i in range(n_workers):\n",
    "\n",
    "                batches.append(random.sample(records, texts_per_worker))\n",
    "\n",
    "            # Send the batches\n",
    "            mean_embedding_time=helper_funcs.submit_batches(worker_gpus, batches)\n",
    "\n",
    "            # Calculate the embedding rate\n",
    "            embedding_rate=target_texts / mean_embedding_time\n",
    "\n",
    "            # Add it to the results\n",
    "            results[f'{n_workers}'].append(embedding_rate)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_multiworker_benchmark == True:\n",
    "\n",
    "    plt.title('Embedding rate benchmark: workers per single GPU')\n",
    "    plt.xlabel('GPU workers')\n",
    "    plt.ylabel('Embedding rate (records per second)')\n",
    "\n",
    "    standard_deviations=[]\n",
    "    means=[]\n",
    "\n",
    "    for n_workers in gpu_worker_counts:\n",
    "        times=results[f'{n_workers}']\n",
    "        means.append(np.mean(times))\n",
    "        standard_deviations.append(np.std(times))\n",
    "\n",
    "    plt.errorbar(\n",
    "        gpu_worker_counts, \n",
    "        means, \n",
    "        yerr=standard_deviations, \n",
    "        linestyle='dotted',\n",
    "        marker='o', \n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "    plt.savefig(f'{figure_dir}/2.3-embedding_rate_workers_per_GPU.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    mean_embedding_rate=sum(results['8']) / len(results['8'])\n",
    "    print(f'Estimated total embedding time: {(estimated_total_chunks / mean_embedding_rate) / (60*60*24):.2f} days with 8 workers')\n",
    "    print(f'Mean embedding rate: {mean_embedding_rate:.0f} records per second with 8 workers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so - it *looks* like we are speeding up the embedding by using multiple workers. But, in reality, even the fastest multi-worker embedding rate is slower than that of a single process. Probably because of the overhead associated with running multiple workers.\n",
    "\n",
    "Let's try it with one worker on each of the three GPUs in the system.\n",
    "\n",
    "## 3. Single worker on multiple GPUs\n",
    "### 3.1. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_multigpu_benchmark == True:\n",
    "        \n",
    "    # Build the GPU list\n",
    "    worker_gpus=['cuda:0','cuda:1','cuda:2']\n",
    "    n_workers=len(worker_gpus)\n",
    "\n",
    "    # Holder for results\n",
    "    results=[]\n",
    "\n",
    "    # Calculate how many texts we need to send to each worker\n",
    "    # the get the target number of texts\n",
    "    texts_per_worker=target_texts // n_workers\n",
    "\n",
    "    for replicate in range(replicates):\n",
    "\n",
    "        # Generate random batches of texts for each worker\n",
    "        batches=[]\n",
    "\n",
    "        for i in range(n_workers):\n",
    "\n",
    "            batches.append(random.sample(records, texts_per_worker))\n",
    "\n",
    "        # Send the batches\n",
    "        mean_embedding_time=helper_funcs.submit_batches(worker_gpus, batches)\n",
    "\n",
    "        # Calculate the embedding rate\n",
    "        embedding_rate=target_texts / mean_embedding_time\n",
    "\n",
    "        # Add it to the results\n",
    "        results.append(embedding_rate)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_multigpu_benchmark == True:\n",
    "    \n",
    "    mean_embedding_rate=sum(results) / len(results)\n",
    "    print(f'Estimated total embedding time: {(estimated_total_chunks / mean_embedding_rate) / (60*60*24):.2f} days')\n",
    "    print(f'Mean embedding rate: {mean_embedding_rate:.0f} records per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that worked somewhat - using all three GPUs, we can cut about half of a day off of the encoding time. However, this approach is not great - the GTX1070 is much faster than the K80s so it sits around waiting for the slower cards to finish each round. This effect could be more pronounced during the real run where each GPU will be embedding hundreds of thousands of articles. To really do this right, we would need a queue system where persistent workers can consume smaller batches as needed until all of the text has been embedded. Keep the GPUs fed!\n",
    "\n",
    "## 4. Single queue-fed worker on multiple GPUs\n",
    "\n",
    "Here is what the set-up looks like.\n",
    "\n",
    "1. Create reader queue to take jobs from reader process to GPU workers.\n",
    "2. Reader process generates batches of text and puts them in the queue.\n",
    "3. GPU workers take jobs from the queue and embed them.\n",
    "\n",
    "For the real implementation, we would also need a writer queue and process to take embedded texts from the GPU workers and write them to disk - hdf5 doesn't like multiple workers writing to the same file.\n",
    "\n",
    "Timing this for benchmarking is also tricky. We can't time in the workers like we did for the batch rounds because the workers won't necessarily be in sync. I think the trick is to start the GPU workers up. Then start the timer, then start the reader process and call join on the GPU workers. The join will block until all of the GPU workers return. Then we stop the timer. Since the work does not start until the reader process starts putting batches in the queue, this gives the GPU workers a chance to spin up before we start timing.\n",
    "\n",
    "### 4.1. Benchmark specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number texts to encode for each replicate of each worker count\n",
    "target_texts=12000\n",
    "\n",
    "# Number of texts to send for one workunit\n",
    "batch_sizes=[1,10,100,1000]\n",
    "\n",
    "# Number of replicates to run\n",
    "replicates=3\n",
    "\n",
    "# Build holder for results\n",
    "results={}\n",
    "\n",
    "# GPU list\n",
    "worker_gpus=['cuda:0','cuda:1','cuda:2']\n",
    "n_workers=len(worker_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_multigpu_queue_benchmark == True:\n",
    "\n",
    "    # Loop on batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "\n",
    "        # Add an empty list to collect the results, using the batch size as key\n",
    "        results[f'{batch_size}']=[]\n",
    "\n",
    "        # Loop on the replicates\n",
    "        for replicate in range(replicates):\n",
    "            print(f'Running replicate {replicate}, batch size {batch_size}')\n",
    "\n",
    "            # Start multiprocessing manager\n",
    "            manager=Manager()\n",
    "\n",
    "            # Set-up reader queue\n",
    "            reader_queue=manager.Queue(maxsize=10)\n",
    "\n",
    "            # Set-up the reader process\n",
    "            reader_process=Process(\n",
    "                target=helper_funcs.reader,\n",
    "                args=(records, target_texts, batch_size, reader_queue, n_workers)\n",
    "            )\n",
    "\n",
    "            # Start the pool\n",
    "            pool=mp.Pool(processes=len(worker_gpus))\n",
    "\n",
    "            # Start each GPU worker\n",
    "            for gpu in worker_gpus:\n",
    "                pool.apply_async(helper_funcs.calculate_embeddings_from_queue, (gpu,reader_queue,))\n",
    "\n",
    "            # Wait for the workers to spin up\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Start the timer\n",
    "            start_time=time.time()\n",
    "\n",
    "            # Start the reader process\n",
    "            reader_process.start()\n",
    "\n",
    "            # Wait for the GPU worker pool to finish\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            # Stop the timer\n",
    "            dT=time.time() - start_time\n",
    "\n",
    "            # Collect the apparent embedding rate\n",
    "            results[f'{batch_size}'].append(target_texts / dT)\n",
    "            \n",
    "            # Clean up\n",
    "            reader_process.close()\n",
    "            manager.shutdown()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_multigpu_queue_benchmark == True:\n",
    "\n",
    "    plt.title('Embedding rate benchmark: queued workunit size')\n",
    "    plt.xlabel('Records per workunit')\n",
    "    plt.ylabel('Rate (records per second)')\n",
    "\n",
    "    standard_deviations=[]\n",
    "    means=[]\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        times=results[f'{batch_size}']\n",
    "        means.append(np.mean(times))\n",
    "        standard_deviations.append(np.std(times))\n",
    "\n",
    "    plt.errorbar(\n",
    "        batch_sizes, \n",
    "        means, \n",
    "        yerr=standard_deviations, \n",
    "        linestyle='dotted',\n",
    "        marker='o', \n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(f'{figure_dir}/4.3-embedding_rate_workunit_size.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    mean_embedding_rate=sum(results['1']) / len(results['1'])\n",
    "    print(f'Estimated total embedding time: {(estimated_total_chunks / mean_embedding_rate) / (60*60*24):.2f} days with workunit size 1.')\n",
    "    print(f'Mean embedding rate: {mean_embedding_rate:.0f} records per second with workunit size 1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! That worked great - two hours of work just saved us a day and a half of embedding time over the first single process version of this. I think we have squeezed out all of the performance we are going to get, time to move on\n",
    "\n",
    "**One final note**: we should probably be using this queue pattern for everything we do with multiprocessing. Workers sitting around while the pool.join() or results.get() call waits for everyone to finish kills the run time. And stopping and restarting workers is obviously not good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
