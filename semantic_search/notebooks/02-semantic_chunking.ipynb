{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic chunking\n",
    "\n",
    "Next, we are going to try using a semantic text splitter to chunk the text to blocks of around 512 tokens.\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We will use semantic-text-splitter from PyPI and the bert base uncased tokenizer from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/opensearch/semantic_search\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_text_splitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# PyPI imports\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_text_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextSplitter\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Internal imports\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'semantic_text_splitter'"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# Standard ports\n",
    "import time\n",
    "\n",
    "# PyPI imports\n",
    "import h5py\n",
    "from semantic_text_splitter import TextSplitter\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=f'{config.DATA_PATH}/wikipedia/{config.BATCHED_TEXT}'\n",
    "\n",
    "tokenizer_name='bert-base-uncased'\n",
    "max_tokens=512\n",
    "\n",
    "tokenizer=Tokenizer.from_pretrained(tokenizer_name)\n",
    "splitter=TextSplitter.from_huggingface_tokenizer(tokenizer, max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "\n",
    "Load up the first batch from the data extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=h5py.File(input_file, 'r')\n",
    "batch=input_data['batches/0']\n",
    "\n",
    "sample_text=' '.join(batch[0].decode('utf-8').split(' ')[:100])\n",
    "\n",
    "print(f'First batch contains {len(batch)} texts')\n",
    "print(f\"First text:\\n{sample_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic chunking test\n",
    "Test split the first text from the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=splitter.chunks(batch[0].decode('utf-8'))\n",
    "print(f'Chunks are: {type(chunks)}')\n",
    "print(f'Have {len(chunks)} chunks')\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk_start=' '.join(chunk.split(' ')[:25])\n",
    "    chunk_end=' '.join(chunk.split(' ')[-25:])\n",
    "    print(f'\\n{chunk_start} ... {chunk_end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, pretty good. We didn't break up any sentences. Some chunks start with a pronoun, so they would be a little unclear to read in isolation. But this approach is obviously much better than taking chunks by word count. Might want to think about a clever way to filter out chunks that only contain references.\n",
    "\n",
    "It feels snappy too - let's time chunking a few batches and see what we are working with.\n",
    "\n",
    "## 4. Semantic text chunking rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Number of batches to time chunking for\n",
    "num_batches=1\n",
    "\n",
    "# Holder to collect chunking times\n",
    "chunking_times=[]\n",
    "\n",
    "for i in range(num_batches):\n",
    "\n",
    "    # Start the timer\n",
    "    start_time=time.time()\n",
    "\n",
    "    # Get the text batch\n",
    "    batch=input_data[f'batches/{i}']\n",
    "\n",
    "    # Chunk the records from the batch\n",
    "    for record in batch:\n",
    "        chunks=splitter.chunks(record.decode('utf-8'))\n",
    "\n",
    "    # Stop the timer\n",
    "    dT=time.time() - start_time\n",
    "    chunking_times.append(dT)\n",
    "\n",
    "mean_chunking_time=sum(chunking_times)/len(chunking_times)\n",
    "print(f'Mean chunking time {mean_chunking_time::.1f} seconds per record\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok here's the estimate: we have 688 batches so about 32 hours to chunk the whole thing.\n",
    "\n",
    "If we parallelize it across our CPU cores, we should be able to get it down to under two hours.\n",
    "\n",
    "## 5. Reference chunk filter\n",
    "\n",
    "Have a clever idea about how to skip chunks which are mostly or all references from the article. Take a look at the following chunk, which contains only references:\n",
    "\n",
    "```text\n",
    "Theatricalia. Retrieved 26 July 2024. \"Black Coffee\". Theatricalia. Retrieved 26 July 2024. \"Alfred Marks\". Theatricalia. Retrieved 26 July 2024. At the Hercule Poirot Central website ... story\". Agatha Christie. 10 October 2017. Retrieved 5 January 2019. \"The Murder on the Links\". latw.org. Retrieved 31 January 2022. \"The Brasserie Ellezelloise's Hercule\". Brasserie-ellezelloise.be.\n",
    "```\n",
    "\n",
    "It contains the word 'Retrieved' with a capitol 'R' many times. I would guess much more than the actual text.\n",
    "\n",
    "Here's another one:\n",
    "\n",
    "```text\n",
    "Christie, Agatha (3 October 2006b). Three Act Tragedy. HarperCollins. ISBN 978-0-06-175403-6. Christie, Agatha (17 March 2009) [1926]. The Murder of Roger Ackroyd. HarperCollins. ISBN 978-0-06-176340-3. Christie, Agatha ... (9 July 2013a). The Lost Mine: A Hercule Poirot Story. HarperCollins. ISBN 978-0-06-229818-8. Christie, Agatha (23 July 2013b). Double Sin: A Hercule Poirot Story. HarperCollins. ISBN 978-0-06-229845-4.\n",
    "```\n",
    "\n",
    "This one doesn't have the 'Retrieved' signal, but it does contain a bunch of ISBNs. Same idea - more than maybe 2 ISBNs means references, not text.\n",
    "\n",
    "We could likely use just those two signals to get rid of a lot of reference chunks. But, now that I'm thinking about it, that breaks our rule that only the text extractor can be tailored to the data source. This is a good idea, but I think we need to try and do it at the level of the extractor.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
