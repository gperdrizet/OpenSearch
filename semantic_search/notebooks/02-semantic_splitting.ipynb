{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic splitting\n",
    "\n",
    "Next, we are going to try using a semantic text splitter to chunk the text to blocks of around 512 tokens. We will use semantic-text-splitter from PyPI and the bert base uncased tokenizer from HuggingFace.\n",
    "\n",
    "**Estimated total run time**: extraction + cleaning + splitting = approximatly 3 hours\n",
    "\n",
    "**TLDR**\n",
    "1. semantic_text_splitter seems to do a fine job of splitting the text into smaller chunks.\n",
    "2. Splitting with a target size of 512 tokens gives us 21 million chunks.\n",
    "3. Splitting the whole corpus with one worker will take about a day, but we can get the net run time under two hours by parallelizing it over batches.\n",
    "\n",
    "## 1. Run set-up\n",
    "\n",
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# Standard ports\n",
    "import time\n",
    "\n",
    "# PyPI imports\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from semantic_text_splitter import TextSplitter\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total line and record counts determined in data exploration notebook\n",
    "line_count=13778448\n",
    "record_count=6889224\n",
    "\n",
    "# Tokenizer & splitter\n",
    "tokenizer_name='bert-base-uncased'\n",
    "max_tokens=512\n",
    "\n",
    "tokenizer=Tokenizer.from_pretrained(tokenizer_name)\n",
    "splitter=TextSplitter.from_huggingface_tokenizer(tokenizer, max_tokens)\n",
    "\n",
    "# Where to save plots\n",
    "figure_dir='./notebooks/figures/02-semantic_splitting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data loading\n",
    "\n",
    "Load up the first batch from the data extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=f'{config.DATA_PATH}/wikipedia-sample/{config.BATCHED_TEXT}'\n",
    "input_data=h5py.File(input_file, 'r')\n",
    "batch=input_data['batches/1']\n",
    "\n",
    "sample_text=' '.join(batch[0].decode('utf-8').split(' ')[:100])\n",
    "\n",
    "print(f'First batch contains {len(batch)} texts\\n')\n",
    "print(f\"Sample text:\\n{sample_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks fine - we still have some character level garbage in there, but that will be cleaned up during the data transform task.\n",
    "\n",
    "## 3. Semantic splitting test\n",
    "\n",
    "Test split the first text from the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=splitter.chunks(batch[0].decode('utf-8'))\n",
    "print(f'Have {len(chunks)} chunks')\n",
    "\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    chunk_start=' '.join(chunk.split(' ')[:25])\n",
    "    chunk_end=' '.join(chunk.split(' ')[-25:])\n",
    "    print(f'\\n{i}: {chunk_start} ... {chunk_end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, pretty good. We didn't break up any sentences. Some chunks start with a pronoun, so they would be a little unclear to read in isolation. But this approach is obviously much better than taking chunks by word count.\n",
    "\n",
    "It feels snappy too - let's time splitting a few batches and see what we are working with.\n",
    "\n",
    "## 4. Semantic splitting rate\n",
    "\n",
    "### 4.1. Benchmark specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of batches to time splitting for\n",
    "num_batches=100\n",
    "\n",
    "# Holder to collect splitting rates\n",
    "splitting_rates=[]\n",
    "\n",
    "# Also, collect the number of chunks we get from each article\n",
    "# so we can get an average at the end\n",
    "chunks_per_article=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(num_batches):\n",
    "\n",
    "    # Start the timer\n",
    "    start_time=time.time()\n",
    "\n",
    "    # Get the text batch\n",
    "    batch=input_data[f'batches/{i + 1}']\n",
    "\n",
    "    # Split the records from the batch\n",
    "    for record in batch:\n",
    "        chunks=splitter.chunks(record.decode('utf-8'))\n",
    "\n",
    "        # Collect the chunk count\n",
    "        chunks_per_article.append(len(chunks))\n",
    "\n",
    "    # Stop the timer\n",
    "    dT=time.time() - start_time\n",
    "\n",
    "    # Collect the split rate\n",
    "    splitting_rates.append(len(batch) / dT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Splitting rate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Semantic splitting rate benchmark')\n",
    "plt.hist(splitting_rates, bins=25)\n",
    "plt.xlabel(f'Mean replicate splitting rate (articles per second)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(f'{figure_dir}/4.3-semantic_splitting_rate.jpg')\n",
    "plt.show()\n",
    "\n",
    "mean_splitting_rate=sum(splitting_rates) / len(splitting_rates)\n",
    "print(f'\\nEstimated total splitting time: {((record_count / mean_splitting_rate) / (60**2)):.1f} hours')\n",
    "print(f'Mean splitting rate {mean_splitting_rate:.1f} records per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Splitting yield results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Semantic splitting yield')\n",
    "plt.hist(chunks_per_article, bins=25)\n",
    "plt.xlabel(f'Chunks per article)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(f'{figure_dir}/4.4-semantic_splitting_yield.jpg')\n",
    "plt.show()\n",
    "\n",
    "mean_chunks_per_article=sum(chunks_per_article) / len(chunks_per_article)\n",
    "estimated_total_chunks=record_count * mean_chunks_per_article\n",
    "print(f'\\nEstimated total chunks: {estimated_total_chunks:.0f}')\n",
    "print(f'Mean chunks per article {mean_chunks_per_article:.1f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Single threaded semantic splitting rate is about 60 records per second or approximately 30 hours to spit all of Wikipedia. Parallelizing that over 18 cores gives us a net splitting time of about an hour and 45 minutes. \n",
    "\n",
    "Combining the semantic splitting time with the net one-hour run time of the parallelized extractor/parser/cleaner functions, and we are looking at best case scenario, about 3 hours for the extraction and parsing steps to complete. All-in-all, not terrible. We can live with overnight, especially considering that the first iteration of this pipeline would have taken over 8 days to run!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
