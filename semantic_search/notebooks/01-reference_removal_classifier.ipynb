{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference removal classifier\n",
    "\n",
    "After some initial semantic chunking experiments, it became apparent that the reference list is included with the text of each article. Unfortunately, there is no section header or any other obvious formatting that could make it easy to remove the reference list. For some articles, as much as 1/3 of the text is references. We need a good way to remove them in the Wikipedia extraction phase or maybe in the semantic chunking phase - I could imagine wanting to do this with other data sources too. Either way, let's see what we can come up with. Here is the plan:\n",
    "\n",
    "1. Split some articles into sentences and manually label them as 'text' or 'reference' to create training data.\n",
    "2. Use Scikit-learn's TFIDF or count vectorizer to prepare the data\n",
    "3. Train a classifier of some sort on it. My thought was XGBoost, but I have seen a few tutorials where people use a Multinomial Naive Bayes model (*sklearn.naive_bayes import MultinomialNB*)\n",
    "\n",
    "Famous last words: sounds like it shouldn't be too hard....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/opensearch/semantic_search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/opensearch/.venv/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/siderealyear/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# PyPI imports\n",
    "import h5py\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=f'{config.DATA_PATH}/wikipedia/{config.BATCHED_TEXT}'\n",
    "training_data_path=f'{config.DATA_PATH}/ref_removal_classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training data preparation\n",
    "\n",
    "This is going to require some manual curation, but hopefully we will not need to many examples. Plan is to sentence split a few records, maybe start with 10, and save them to txt files. Then we can look at each file and copy past the 'text' sentences out into one file and the 'reference' sentences out into another file. Should be pretty easy and quick, will just need to visualy identify where the reference section starts and copy/paste above/below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the hdf5 file and grab the first batch\n",
    "input_data=h5py.File(input_file, 'r')\n",
    "batch=input_data['batches/0']\n",
    "\n",
    "# Loop through 10 records, sentence split and write each to a file\n",
    "for i, record in enumerate(batch[:100]):\n",
    "\n",
    "    # Get the text string from bytes object\n",
    "    record_text=record.decode('utf-8')\n",
    "\n",
    "    # Apply sentence splitter\n",
    "    sentences=nltk.tokenize.sent_tokenize(record_text)\n",
    "\n",
    "    # Join with newline for output to file\n",
    "    sentences='\\n'.join(sentences)\n",
    "\n",
    "    output_file=f'{training_data_path}/{i}.txt'\n",
    "\n",
    "    with open(output_file, 'w') as output:\n",
    "        print(sentences, file=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, went through up to record 30. Let's try with that. We have about 500 lines each of text and reference. Also notices along the way there were sever; 'disambiguation' pages and a few that were just a sentence or two about a movie: '_____ is a film by ____' and then a cast list. Maybe should be filtering those out too?\n",
    "\n",
    "Next, we need to load the two training data files, add the labels, combine and shuffle them, train test split them and then vectorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raj Hans Kumar stated that in political affair...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since the mid-80s, Breydon Water has been a na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Khalsa Diwan Society was founded on July 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TVFPlay.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beaton has also occasionally appeared on Fly T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Editor's Review\", Harvard Educational Review,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CITED: p. 377-378.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eastern Daily Press.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Diaries of A. Christie.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The A.B.C.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Previously, the KDS was controlled by Marxist ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>By the 1960s, he played at the Bel Air Nightcl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ISBN 9780992936440.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Lunar Nomenclature\".</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Short and Leonard J. Waks (Sense Publishers, R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ames, and Inglis, \"Conflict and Change in Brit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Notably, Trevor's Poirot did not have a mousta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Retrieved 2015-08-17.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>At one point the mill supported a small settle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(1999).</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  label\n",
       "0   Raj Hans Kumar stated that in political affair...      0\n",
       "1   Since the mid-80s, Breydon Water has been a na...      0\n",
       "2   The Khalsa Diwan Society was founded on July 2...      0\n",
       "3                                            TVFPlay.      1\n",
       "4   Beaton has also occasionally appeared on Fly T...      0\n",
       "5   \"Editor's Review\", Harvard Educational Review,...      1\n",
       "6                                  CITED: p. 377-378.      1\n",
       "7                                Eastern Daily Press.      1\n",
       "8                         The Diaries of A. Christie.      1\n",
       "9                                          The A.B.C.      1\n",
       "10  Previously, the KDS was controlled by Marxist ...      0\n",
       "11  By the 1960s, he played at the Bel Air Nightcl...      0\n",
       "12                                ISBN 9780992936440.      1\n",
       "13                              \"Lunar Nomenclature\".      1\n",
       "14  Short and Leonard J. Waks (Sense Publishers, R...      0\n",
       "15  Ames, and Inglis, \"Conflict and Change in Brit...      1\n",
       "16  Notably, Trevor's Poirot did not have a mousta...      0\n",
       "17                              Retrieved 2015-08-17.      1\n",
       "18  At one point the mill supported a small settle...      0\n",
       "19                                            (1999).      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the reference and text sentences\n",
    "with open(f'{training_data_path}/ref_sentences.txt') as file:\n",
    "    ref_sentences=[line.rstrip() for line in file]\n",
    "\n",
    "with open(f'{training_data_path}/text_sentences.txt') as file:\n",
    "    text_sentences=[line.rstrip() for line in file]\n",
    "\n",
    "# Create lists of labels\n",
    "ref_labels=[1]*len(ref_sentences)\n",
    "text_labels=[0]*len(text_sentences)\n",
    "\n",
    "# Make dataframes\n",
    "ref_df=pd.DataFrame({\n",
    "    'sentence': ref_sentences,\n",
    "    'label': ref_labels\n",
    "})\n",
    "\n",
    "text_df=pd.DataFrame({\n",
    "    'sentence': text_sentences,\n",
    "    'label': text_labels\n",
    "})\n",
    "\n",
    "# Concatenate\n",
    "data_df=pd.concat([text_df, ref_df])\n",
    "\n",
    "# Shuffle\n",
    "data_df=data_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.20, stratify=data_df.label)\n",
    "\n",
    "# Vectorize with bag-of-words\n",
    "# vec=CountVectorizer(\n",
    "#     ngram_range=(1, 3),\n",
    "#     stop_words='english',\n",
    "# )\n",
    "\n",
    "# Vectorize with TF-IDF\n",
    "vec=TfidfVectorizer()\n",
    "\n",
    "train_features = vec.fit_transform(train_df.sentence)\n",
    "test_features = vec.transform(test_df.sentence)\n",
    "\n",
    "train_labels = train_df.label\n",
    "test_labels = test_df.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.94      0.84       168\n",
      "           1       0.94      0.75      0.84       204\n",
      "\n",
      "    accuracy                           0.84       372\n",
      "   macro avg       0.85      0.85      0.84       372\n",
      "weighted avg       0.86      0.84      0.84       372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=GaussianNB()\n",
    "model.fit(train_features.toarray(), train_labels)\n",
    "\n",
    "predictions=model.predict(test_features.toarray())\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient boosting decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91       168\n",
      "           1       0.92      0.95      0.93       204\n",
      "\n",
      "    accuracy                           0.92       372\n",
      "   macro avg       0.93      0.92      0.92       372\n",
      "weighted avg       0.93      0.92      0.92       372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier()\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "predictions=model.predict(test_features)\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sanity check\n",
    "XGBoost appears to be winning. Let's inspect some predictions and see how we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \"After a careful study of the goods displayed in the window, Poirot entered and represented himself as desirous of purchasing a rucksack for a hypothetical nephew.\"\n",
      "0: A listing of graduates from 1926, lists the first class as 1886.\n",
      "1: \"Career - Theatre 1956-1959\".\n",
      "0: Their style was considered \"modern instrumental\".\n",
      "1: Christie 1939, Chapter 7.\n",
      "0: [citation needed] The initial plans asked for a library and community centre, but these aspects were eliminated from the plans.\n",
      "1: Atlas of the Moon.\n",
      "1: Play.cine.ar.\n",
      "1: Stockholm: Prisma.\n",
      "0: Crazy Women (Spanish: Las locas) is a 1977 Argentine drama film written by José P. Dominiani and directed by Enrique Carreras.\n",
      "1: Retrieved 7 July 2020.\n",
      "0: So much had he become the rage that every rich woman who had mislaid a bracelet or lost a pet kitten rushed to secure the services of the great Hercule Poirot.\n",
      "1: \"Jockey Julien Leparoux Prepares for Biggest Tests\".\n",
      "1: Archived from the original on 22 April 2022.\n",
      "0: He also was member of the American Ornithological Union member and librarian of the Ohio Academy of Science member and president of the Wheaton Ornithological Society member and treasurer of the Columbus Horticultural Society charter member of the American Association of Museums (now the American Alliance of Museums) member of the Columbus Iris Society member of the National Research Council of Archaeology fellow of the American Ethnological Society fellow of the American Association for the Advancement of Science fellow of the American Anthropological Society assistant editor of the Ohio Naturalist lecturer in Sociology in the College of Commerce and Administration of the Ohio State University Mills died in Columbus, Ohio.\n",
      "0: He first met Poirot in Belgium in 1904, during the Abercrombie Forgery.\n",
      "0: It lies in the southwestern part of the mountainous region named the Montes Taurus.\n",
      "0: The history of the countess is, like Poirot's, steeped in mystery.\n",
      "1: ISBN 9789879728017.\n",
      "1: \"The Brasserie Ellezelloise's Hercule\".\n"
     ]
    }
   ],
   "source": [
    "for prediction, sentence in zip(predictions[:20], test_df['sentence'][:20]):\n",
    "    print(f'{prediction}: {sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some obvious misclassifications, for example:\n",
    "\n",
    "```text\n",
    "At low tide there are vast areas of mudflats and saltings, all teeming with birds.\n",
    "```\n",
    "\n",
    "Is definitely not a reference derived fragment, but on the whole we are doing pretty well. I would rather loose a sentence here or there than include more than 50% by reference list content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
