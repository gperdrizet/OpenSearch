{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "Let's take a look at the data we got from the Wikipedia CirrusSearch dump. The gzip 'content' file is 34 GB on disk, so I don't think that we want to try decompressing and reading the whole thing into memory unless we have to. Let's see if we can stream the data from the gzip archive and take a look at what we have.\n",
    "\n",
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gzip import GzipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_records=5\n",
    "data_file_path='./data/enwiki-20240930-cirrussearch-content.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a few records for inspection\n",
    "file_stream=GzipFile(data_file_path)\n",
    "records = []\n",
    "\n",
    "for i in range(sample_records):\n",
    "\n",
    "    line=next(file_stream)\n",
    "    record=json.loads(line)\n",
    "    records.append(record)\n",
    "\n",
    "print(f'Loaded {len(records)} records from gzip archive.')\n",
    "print(f'Record is: {type(records[0])}')\n",
    "\n",
    "print(f'\\nRecord 0 contains:')\n",
    "\n",
    "for key, value in records[0].items():\n",
    "    print(f' {key}: {value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks like the first line in the file is just some metadata. Let's look at the second record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nRecord 1 keys:')\n",
    "\n",
    "for key in records[1].keys():\n",
    "    print(f' {key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More like what we were expecting. We have keys for title, text, timestamp, language, even popularity score? didn't know Wikipedia had that. Here is the title and some of the text from the first record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = ' '.join(records[1]['text'].split(' ')[:100])\n",
    "\n",
    "print(f\"Title: {records[1]['title']}\")\n",
    "print(f'Text\\n{text_sample}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that's a Wikipedia article. Good, I think we can work with this. Since the unzipped files are too big to fit in memory, let's first see if there is any time benefit to streaming the data from the gzip archive vs the decompressed JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Record read rate\n",
    "We have two options to stream the data. \n",
    "1. From the gzip archive\n",
    "2. From the decompressed JSON\n",
    "Which one is faster? By how much? There is obviously an advantage to reading straight from the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "file_stream=GzipFile(data_file_path)\n",
    "\n",
    "record_count=0\n",
    "no_text_records=0\n",
    "text_word_counts=[]\n",
    "\n",
    "for line in file_stream:\n",
    "\n",
    "    record_count+=1\n",
    "    record=json.loads(line)\n",
    "    \n",
    "    if 'text' in record.keys():\n",
    "        article_text=record['text']\n",
    "\n",
    "        text_word_count=len(article_text.split(' '))\n",
    "        text_word_counts.append(text_word_count)\n",
    "\n",
    "    else:\n",
    "        no_text_records+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while this is running bond0 is only seeing 14-15 MiB per second from the array. This means to visit the whole file, assuming ~160 GB data total could take up to 22 hrs! Maybe we should try decompressing it first for processing. Regardless, this is a huge bottle-neck. Will definitely want to parallelize the data preparation as much as possible. Think the strategy should be the following:\n",
    "\n",
    "1. Definitely use the fast scratch NVMe SSD, not the NFS RAID.\n",
    "2. Decompress the file using linux's gzip.\n",
    "4. Split the file with the linux split command.\n",
    "\n",
    "At this point we need to think about what format we want to keep intermediate data it. HDF5 via h5py supports many readers operating on the same file out of the box. To run multiple writers, you need to use mpi4py and compile HDF5 and h5py with MPI support. I would still like to end up with an HDF5 file at the end of the pre-processing, even if it contains multiple batches. But, given the complexity of multiple writers it may make more sense to shard data as parquet or something. This might deserve a little more research.\n",
    "\n",
    "Ok, let's do it this way:\n",
    "\n",
    "5. Read the chunks in parallel.\n",
    "6. Extract title and text.\n",
    "7. Semantically chunk text down to ~512 tokens per chunk.\n",
    "8. Save processed batches as parquet.\n",
    "\n",
    "Looks like (semantic-text-splitter)[https://pypi.org/project/semantic-text-splitter/] will be the way to do the chunking. Don't know how fast it will be, but we can always fall back on dumb word count splitting if need be.\n",
    "\n",
    "OK, think we have a good plan and some work to do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
