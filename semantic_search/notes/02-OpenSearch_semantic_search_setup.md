# OpenSearch semantic search setup

Need to prepare the cluster to run the embedding model. Could maybe add this as this to the Luigi data pipeline later, but we only need to do it once per cluster, so for now, let's just set everything up manually via the OpenSearch dashboard dev tools console.
Basic cluster settings and model deployment adapted from OpenSearch's [neural search tutorial](https://opensearch.org/docs/latest/search-plugins/neural-search-tutorial/).

## 1. Cluster set-up

### 1.1. GPU support

Pre-requisite: nvidia-container-toolkit. Set-up instructions in [Docker Documentation](https://docs.docker.com/engine/containers/resource_constraints/#gpu). Then add the following to the node stanza(s) in the cluster's docker-compose file:

```yaml
deploy:
    resources:
    reservations:
        devices:
        - driver: nvidia
        device_ids: ['2']
        capabilities: [gpu]
```

Verify GPU access from inside the container(s) by attaching to a terminal and checking the output of *nvidia-smi*:

```text
$ docker exec -it opensearch-node1 sh
$ nvidia-smi

Sun Oct 13 11:43:19 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:07:00.0 Off |                  N/A |
|  0%   50C    P8    10W / 151W |   4985MiB /  8118MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
```

### 1.2. Machine learning node

Since we don't have a dedicated ML node and the cluster is running all on the same machine we need to set the model to not look for a dedicated ML node. Send the following via the OpenSearch console:

```text
PUT _cluster/settings
{
  "persistent": {
    "plugins": {
      "ml_commons": {
        "only_run_on_ml_node": "false",
        "model_access_control_enabled": "true",
        "native_memory_threshold": "99"
      }
    }
  }
}
```

Response:

```text
{
  "acknowledged": true,
  "persistent": {
    "plugins": {
      "ml_commons": {
        "only_run_on_ml_node": "false",
        "model_access_control_enabled": "true",
        "native_memory_threshold": "99"
      }
    }
  },
  "transient": {}
}
```

## 2. Set up the embedding model

We will use DistilBERT model as described in the tutorial as a starting point, but other models [are available](https://opensearch.org/docs/latest/ml-commons-plugin/using-ml-models/)

### 2.1. Register a model group

Omit the access control parameter because we are running with the security plugin disabled.

```text
POST /_plugins/_ml/model_groups/_register
{
  "name": "NLP_model_group",
  "description": "Model group for vector search embedding models"
}
```

Response:

```text
{
  "model_group_id": "UcG3hZIBe4T97LPedcgx",
  "status": "CREATED"
}
```

Add the autogenerated *model_group_id* to the ETL pipeline configuration file.

### 2.2. Register the model

Use the autogenerated *model_group_id* from the last step.

```text
POST /_plugins/_ml/models/_register
{
  "name": "huggingface/sentence-transformers/msmarco-distilbert-base-tas-b",
  "version": "1.0.1",
  "model_group_id": "UcG3hZIBe4T97LPedcgx",
  "model_format": "TORCH_SCRIPT"
}
```

Response:

```text
{
  "task_id": "UsG7hZIBe4T97LPe38iN",
  "status": "CREATED"
}
```

Check the task status with the task ID returned in the previous step:

```text
GET /_plugins/_ml/tasks/UsG7hZIBe4T97LPe38iN
```

Once the task is complete, you should receive output like this:

{
  "model_id": "S5-7hZIByzOSp7wU4wxw",
  "task_type": "REGISTER_MODEL",
  "function_name": "TEXT_EMBEDDING",
  "state": "COMPLETED",
  "worker_node": [
    "V-kSNM2jS7W4YE5yzQOp3Q"
  ],
  "create_time": 1728820534989,
  "last_update_time": 1728820571857,
  "is_async": true
}

Add the autogenerated *model_id* to the ETL pipeline configuration file, and then check that the model was loaded correctly:

```text
GET /_plugins/_ml/models/S5-7hZIByzOSp7wU4wxw
```

Response:

```text
{
  "name": "huggingface/sentence-transformers/msmarco-distilbert-base-tas-b",
  "model_group_id": "UcG3hZIBe4T97LPedcgx",
  "algorithm": "TEXT_EMBEDDING",
  "model_version": "1",
  "model_format": "TORCH_SCRIPT",
  "model_state": "REGISTERED",
  "model_content_size_in_bytes": 266352827,
  "model_content_hash_value": "acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413",
  "model_config": {
    "model_type": "distilbert",
    "embedding_dimension": 768,
    "framework_type": "SENTENCE_TRANSFORMERS",
    "all_config": """{"_name_or_path":"old_models/msmarco-distilbert-base-tas-b/0_Transformer","activation":"gelu","architectures":["DistilBertModel"],"attention_dropout":0.1,"dim":768,"dropout":0.1,"hidden_dim":3072,"initializer_range":0.02,"max_position_embeddings":512,"model_type":"distilbert","n_heads":12,"n_layers":6,"pad_token_id":0,"qa_dropout":0.1,"seq_classif_dropout":0.2,"sinusoidal_pos_embds":false,"tie_weights_":true,"transformers_version":"4.7.0","vocab_size":30522}"""
  },
  "created_time": 1728820535987,
  "last_updated_time": 1728820571813,
  "last_registered_time": 1728820571812,
  "total_chunks": 27,
  "is_hidden": false
}
```

### 2.3. Deploy the model

Deploying loads a model instance into memory so that it is ready to use.

```text
POST /_plugins/_ml/models/S5-7hZIByzOSp7wU4wxw/_deploy
```

Response:

```text
{
  "task_id": "VMHBhZIBe4T97LPeXsgZ",
  "task_type": "DEPLOY_MODEL",
  "status": "CREATED"
}
```

Check the task status:

```text
GET /_plugins/_ml/tasks/VMHBhZIBe4T97LPeXsgZ
```

Response:

{
  "model_id": "S5-7hZIByzOSp7wU4wxw",
  "task_type": "DEPLOY_MODEL",
  "function_name": "TEXT_EMBEDDING",
  "state": "COMPLETED",
  "worker_node": [
    "V-kSNM2jS7W4YE5yzQOp3Q",
    "XOmkA1cpRUKy3FidFpgXwg",
    "P9xmTQi5SS2e6uM7rI1BgA"
  ],
  "create_time": 1728820895256,
  "last_update_time": 1728820895350,
  "is_async": true
}

The model state should now show up as 'deployed':

```text
GET /_plugins/_ml/models/S5-7hZIByzOSp7wU4wxw
```

Response:

```text
{
  "name": "huggingface/sentence-transformers/msmarco-distilbert-base-tas-b",
  "model_group_id": "UcG3hZIBe4T97LPedcgx",
  "algorithm": "TEXT_EMBEDDING",
  "model_version": "1",
  "model_format": "TORCH_SCRIPT",
  "model_state": "DEPLOYED",
  "model_content_size_in_bytes": 266352827,
  "model_content_hash_value": "acdc81b652b83121f914c5912ae27c0fca8fabf270e6f191ace6979a19830413",
  "model_config": {
    "model_type": "distilbert",
    "embedding_dimension": 768,
    "framework_type": "SENTENCE_TRANSFORMERS",
    "all_config": """{"_name_or_path":"old_models/msmarco-distilbert-base-tas-b/0_Transformer","activation":"gelu","architectures":["DistilBertModel"],"attention_dropout":0.1,"dim":768,"dropout":0.1,"hidden_dim":3072,"initializer_range":0.02,"max_position_embeddings":512,"model_type":"distilbert","n_heads":12,"n_layers":6,"pad_token_id":0,"qa_dropout":0.1,"seq_classif_dropout":0.2,"sinusoidal_pos_embds":false,"tie_weights_":true,"transformers_version":"4.7.0","vocab_size":30522}"""
  },
  "created_time": 1728820535987,
  "last_updated_time": 1728820895322,
  "last_registered_time": 1728820571812,
  "last_deployed_time": 1728820895322,
  "auto_redeploy_retry_times": 0,
  "total_chunks": 27,
  "planning_worker_node_count": 3,
  "current_worker_node_count": 3,
  "planning_worker_nodes": [
    "V-kSNM2jS7W4YE5yzQOp3Q",
    "XOmkA1cpRUKy3FidFpgXwg",
    "P9xmTQi5SS2e6uM7rI1BgA"
  ],
  "deploy_to_all_nodes": true,
  "is_hidden": false
}
```

### 2.4. Model auto-redeployment

Set the model to automatically re-deploy on cluster crash or restart:

```text
PUT _cluster/settings
{
  "persistent": {
    "plugins.ml_commons.model_auto_deploy.enable": "false"
  }
}
```

Response:

```text
{
  "acknowledged": true,
  "persistent": {
    "plugins": {
      "ml_commons": {
        "model_auto_deploy": {
          "enable": "true"
        }
      }
    }
  },
  "transient": {}
}
```

## 3. Data ingest

Here we need two things - an ingest pipeline and an index. The ingest pipeline can be set via the OpenSearch dashboard dev tools console. The index creation we will do via the ETL pipeline so we can create a specific index for whatever dataset we are loading.

### 3.1. Neural search ingest pipeline

The ingest pipeline maps a 'text' field to a 'text_embedding' field using the model we just deployed. Make sure to set the token limit equal or greater than the chunk size used in the ETL pipeline. We are using 512 for testing. Also update the model ID to use the model we just deployed.

```text
PUT /_ingest/pipeline/embedding-ingest-pipeline
{
  "description": "An vector search embedding pipeline",
  "processors": [
    {
      "text_embedding": {
        "model_id": "S5-7hZIByzOSp7wU4wxw",
        "field_map": {
          "text": "text_embedding"
        }
      }
    }
  ]
}
```

Response:

```text
{
  "acknowledged": true
}
```

Add the pipeline ID to ETL configuration file.

### 3.2. Vector search index

This will be done via the ETL pipeline, adding the stanza here for reference. Make sure to set the default pipeline to the pipeline created above. The field names must also match those in the ingest pipeline mapping.

```text
{
  "settings": {
    "number_of_shards": 3,
    "index.knn": "true",
    "default_pipeline": f'{config.INGEST_PIPELINE_ID}'
  },
  "mappings": {
    "properties": {
      "text_embedding": {
        "type": "knn_vector",
        "dimension": 768,
        "method": {
          "engine": "lucene",
          "space_type": "l2",
          "name": "hnsw",
          "parameters": {}
        }
      },
      "text": {
        "type": "text"
      }
    }
  }
}
```
